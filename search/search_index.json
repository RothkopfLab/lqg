{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LQG: Inverse Optimal Control for Continuous Psychophysics","text":"<p> <p> </p> <p></p> <p>This repository contains the official JAX implementation of the inverse optimal control method presented in the paper:</p> <p>Straub, D., &amp; Rothkopf, C. A. (2022). Putting perception into action with inverse optimal control for continuous psychophysics. eLife, 11, e76635.</p>"},{"location":"#installation","title":"Installation","text":"<p>The package can be installed via <code>pip</code></p> <pre><code>python -m pip install lqg\n</code></pre> <p>Since publication of our eLife paper, I have substantially updated the package. If you want to use the package as described in the paper, please install an older version <code>&lt;0.20</code>:</p> <pre><code>python -m pip install lqg==0.1.9\n</code></pre> <p>If you want the latest development version, I recommend cloning the repository and installing locally in a virtual environment: </p> <pre><code>git clone git@github.com:dominikstrb/lqg.git\ncd lqg\npython -m venv env\nsource env/bin/activate\npython -m pip install -e .\n</code></pre>"},{"location":"#usage-examples","title":"Usage examples","text":"<p>The notebooks in the documentation illustrate how to use the <code>lqg</code> package to define optimal control models, simulate trajectories, and infer parameters from observed data. - <code>Overview</code> explains the model and its parameters in more detail, including the extension to subjective internal models (based on my tutorial at CCN 2022) - <code>Data</code> applies the method to data from a tracking experiment</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use our method or code in your research, please cite our paper:</p> <pre><code>@article{straub2022putting,\n  title={Putting perception into action with inverse optimal control for continuous psychophysics},\n  author={Straub, Dominik and Rothkopf, Constantin A},\n  journal={eLife},\n  volume={11},\n  pages={e76635},\n  year={2022},\n  publisher={eLife Sciences Publications Limited}\n}\n</code></pre>"},{"location":"#extensions","title":"Extensions","text":""},{"location":"#signal-dependent-noise","title":"Signal-dependent noise","text":"<p>This implementation supports the basic LQG framework. For the extension to signal-dependent noise (Todorov, 2005), please see our NeurIPS 2021 paper and its implementation.</p>"},{"location":"#non-linear-dynamics","title":"Non-linear dynamics","text":"<p>We are currently working on extending this approach to non-linear dynamics and non-quadratic costs. Please check out our NeurIPS 2023 paper and its implementation.</p>"},{"location":"api/","title":"API","text":"<p>Under construction</p> <p>Currently, no API documentation exists. For the time being we recommend using the GitHub code viewer.</p> <p>Go to GitHub</p>"},{"location":"tutorials/","title":"Tutorials","text":"<ul> <li> <p> Overview</p> <p>A showcase of the core functionality and use cases of <code>lqg</code>.</p> </li> <li> <p> Data</p> <p>Brief demonstration of tracking data evaluation.</p> </li> </ul>"},{"location":"tutorials/data/","title":"Data","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport arviz as az\n\nfrom lqg.io import load_tracking_data\n\ndata, blob_widths = load_tracking_data(data_path=\"../../data/\")\ndata.shape\n</pre> import matplotlib.pyplot as plt import arviz as az  from lqg.io import load_tracking_data  data, blob_widths = load_tracking_data(data_path=\"../../data/\") data.shape Out[1]: <pre>(6, 20, 1068, 2)</pre> In\u00a0[2]: Copied! <pre>plt.plot(data[0, 0, :, 0])\nplt.plot(data[0, 0, :, 1])\n\nplt.show()\n</pre> plt.plot(data[0, 0, :, 0]) plt.plot(data[0, 0, :, 1])  plt.show() In\u00a0[3]: Copied! <pre>seed = 1\nmodels = [\"BoundedActor\", \"SubjectiveActor\"]\n\nresults = {model_name: az.from_netcdf(f\"../../data/processed/{model_name}-{seed}.nc\") for model_name in models}\n</pre> seed = 1 models = [\"BoundedActor\", \"SubjectiveActor\"]  results = {model_name: az.from_netcdf(f\"../../data/processed/{model_name}-{seed}.nc\") for model_name in models} In\u00a0[4]: Copied! <pre>for model_name, inference_data in sorted(results.items()):\n    summary = az.summary(inference_data.posterior, var_names=[f\"sigma_{i}\" for i in range(6)])\n    display(summary)\n\n    plt.plot(blob_widths, summary[\"mean\"], label=model_name)\n    plt.fill_between(blob_widths, summary[\"hdi_3%\"], summary[\"hdi_97%\"], alpha=0.5)\n    plt.ylim(0, )\n\nplt.legend()\n\nplt.show()\n</pre> for model_name, inference_data in sorted(results.items()):     summary = az.summary(inference_data.posterior, var_names=[f\"sigma_{i}\" for i in range(6)])     display(summary)      plt.plot(blob_widths, summary[\"mean\"], label=model_name)     plt.fill_between(blob_widths, summary[\"hdi_3%\"], summary[\"hdi_97%\"], alpha=0.5)     plt.ylim(0, )  plt.legend()  plt.show() mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat sigma_0 8.544 0.273 8.032 9.056 0.002 0.001 22415.0 14534.0 1.0 sigma_1 9.726 0.337 9.104 10.373 0.002 0.002 21472.0 15742.0 1.0 sigma_2 11.812 0.360 11.147 12.502 0.002 0.002 20882.0 14644.0 1.0 sigma_3 19.854 0.918 18.193 21.622 0.006 0.004 23440.0 15436.0 1.0 sigma_4 28.512 1.516 25.677 31.347 0.011 0.008 19540.0 14415.0 1.0 sigma_5 51.588 3.234 45.550 57.624 0.024 0.017 18498.0 16077.0 1.0 mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat sigma_0 11.844 0.390 11.126 12.590 0.003 0.002 17120.0 14474.0 1.0 sigma_1 12.188 0.419 11.396 12.956 0.003 0.002 19297.0 14892.0 1.0 sigma_2 14.951 0.548 13.965 16.028 0.004 0.003 18766.0 15385.0 1.0 sigma_3 19.438 0.899 17.733 21.098 0.007 0.005 19041.0 14257.0 1.0 sigma_4 27.277 1.390 24.766 30.001 0.010 0.007 18600.0 14759.0 1.0 sigma_5 49.542 3.561 43.016 56.399 0.026 0.019 18428.0 15201.0 1.0 In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/data/#data","title":"Data\u00b6","text":""},{"location":"tutorials/overview/","title":"Overview","text":"In\u00a0[1]: Copied! <pre>import matplotlib as mpl\nimport matplotlib.pyplot as plt\n#from ipywidgets import interact, widgets\n\nmpl.rcParams['axes.spines.right'] = False\nmpl.rcParams['axes.spines.top'] = False\n\nimport altair as alt\nimport arviz as az\nimport numpyro\nimport pandas as pd\nfrom jax import grad, jit\nfrom jax import numpy as jnp\nfrom jax import random, vmap\nfrom numpyro import distributions as dist\nfrom numpyro.infer import MCMC, NUTS\n\nfrom lqg import LQG, Actor, Dynamics, System, xcorr\n</pre> import matplotlib as mpl import matplotlib.pyplot as plt #from ipywidgets import interact, widgets  mpl.rcParams['axes.spines.right'] = False mpl.rcParams['axes.spines.top'] = False  import altair as alt import arviz as az import numpyro import pandas as pd from jax import grad, jit from jax import numpy as jnp from jax import random, vmap from numpyro import distributions as dist from numpyro.infer import MCMC, NUTS  from lqg import LQG, Actor, Dynamics, System, xcorr In\u00a0[2]: Copied! <pre># We have to use DataFrames that a larger than recommended -&gt; turn off the error\nalt.data_transformers.disable_max_rows()\n</pre> # We have to use DataFrames that a larger than recommended -&gt; turn off the error alt.data_transformers.disable_max_rows() Out[2]: <pre>DataTransformerRegistry.enable('default')</pre> In\u00a0[3]: Copied! <pre>a = jnp.array([[1., 2.],\n               [3., 4.]])\n</pre> a = jnp.array([[1., 2.],                [3., 4.]]) In\u00a0[4]: Copied! <pre>key = random.PRNGKey(1)\nb = random.normal(key, shape=(2, 3))\n\na @ b\n</pre> key = random.PRNGKey(1) b = random.normal(key, shape=(2, 3))  a @ b Out[4]: <pre>Array([[ 1.6896877, -0.6240323,  1.5889112],\n       [ 4.3366823, -2.2179935,  4.184889 ]], dtype=float32)</pre> In\u00a0[5]: Copied! <pre>def f(x):\n    return jnp.sin(x)\n\ngrad(f)(jnp.pi)\n</pre> def f(x):     return jnp.sin(x)  grad(f)(jnp.pi) Out[5]: <pre>Array(-1., dtype=float32, weak_type=True)</pre> In\u00a0[6]: Copied! <pre>x = jnp.linspace(0, 2 * jnp.pi)\n</pre> x = jnp.linspace(0, 2 * jnp.pi) In\u00a0[7]: Copied! <pre>plt.plot(x, f(x))\nplt.plot(x, vmap(grad(f))(x))\n</pre> plt.plot(x, f(x)) plt.plot(x, vmap(grad(f))(x)) Out[7]: <pre>[&lt;matplotlib.lines.Line2D at 0x7feb443178e0&gt;]</pre> In\u00a0[8]: Copied! <pre>def f(x):\n    return x * x + x * 2.0\n\nx = jnp.ones((5000, 5000))\n</pre> def f(x):     return x * x + x * 2.0  x = jnp.ones((5000, 5000)) In\u00a0[9]: Copied! <pre>%timeit f(x)\n</pre> %timeit f(x) <pre>63 ms \u00b1 1.7 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</pre> In\u00a0[10]: Copied! <pre>jit_f = jit(f)\n%timeit jit_f(x)\n</pre> jit_f = jit(f) %timeit jit_f(x) <pre>22.8 ms \u00b1 1.91 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</pre> In\u00a0[11]: Copied! <pre>action_variability = 0.5\nsigma_target = 6.\nsigma_cursor = 1.\naction_cost = .05\n\ndt = 1. / 60.\n\n# dynamical system\nA = jnp.eye(2)\nB = jnp.array([[0.], \n              [dt]])\n\n# noise\nV = jnp.diag(jnp.array([1., action_variability]))\n\n# observation model\nC = jnp.eye(2)\nW = jnp.diag(jnp.array([sigma_target, sigma_cursor]))\n\n# cost function\nQ = jnp.array([[1., -1.],\n              [-1., 1]])\n\nR = jnp.eye(1) * action_cost\n\nT = 500\nmodel = LQG(A, B, C, V, W, Q, R, T=T)\n</pre> action_variability = 0.5 sigma_target = 6. sigma_cursor = 1. action_cost = .05  dt = 1. / 60.  # dynamical system A = jnp.eye(2) B = jnp.array([[0.],                [dt]])  # noise V = jnp.diag(jnp.array([1., action_variability]))  # observation model C = jnp.eye(2) W = jnp.diag(jnp.array([sigma_target, sigma_cursor]))  # cost function Q = jnp.array([[1., -1.],               [-1., 1]])  R = jnp.eye(1) * action_cost  T = 500 model = LQG(A, B, C, V, W, Q, R, T=T) <p>The <code>System</code> class, which <code>LQG</code> extends, can be pretty-printed. When calling <code>display(model)</code>, the matrices of your model will be displayed as $\\LaTeX$ math formula. The same happens when the output of a cell is printed. Standard (i.e., not so pretty) printing can still be achieved through <code>print(model)</code>.</p> In\u00a0[12]: Copied! <pre>model  # equivalent to `display(model)`\n</pre> model  # equivalent to `display(model)` Out[12]:  \\begin{align*} \\text{Dynamics:} &amp;&amp;A = \\begin{bmatrix} 1. &amp; 0.\\\\ 0. &amp; 1.\\\\\\end{bmatrix} &amp;&amp;B = \\begin{bmatrix} 0.\\\\ 0.0167\\\\\\end{bmatrix} &amp;&amp;F = \\begin{bmatrix} 1. &amp; 0.\\\\ 0. &amp; 1.\\\\\\end{bmatrix} &amp;&amp;V = \\begin{bmatrix} 1. &amp; 0.\\\\ 0. &amp; 0.5\\\\\\end{bmatrix} &amp;&amp;W = \\begin{bmatrix} 6. &amp; 0.\\\\ 0. &amp; 1.\\\\\\end{bmatrix}\\\\\\text{Actor:} &amp;&amp;A = \\begin{bmatrix} 1. &amp; 0.\\\\ 0. &amp; 1.\\\\\\end{bmatrix} &amp;&amp;B = \\begin{bmatrix} 0.\\\\ 0.0167\\\\\\end{bmatrix} &amp;&amp;F = \\begin{bmatrix} 1. &amp; 0.\\\\ 0. &amp; 1.\\\\\\end{bmatrix} &amp;&amp;V = \\begin{bmatrix} 1. &amp; 0.\\\\ 0. &amp; 0.5\\\\\\end{bmatrix} &amp;&amp;W = \\begin{bmatrix} 6. &amp; 0.\\\\ 0. &amp; 1.\\\\\\end{bmatrix} &amp;&amp;Q = \\begin{bmatrix} 1. &amp; -1.\\\\ -1. &amp; 1.\\\\\\end{bmatrix} &amp;&amp;R = \\begin{bmatrix} 0.05\\\\\\end{bmatrix}\\end{align*}  <p>Let's simulate some tracking data by applying the Kalman filter and linear-quadratic regular. This is implemented in the method <code>simulate(rng_key, n, T)</code>. Since <code>jax</code> does not have a global random number generator state, we need to pass a <code>PRNGKey</code> object. <code>n</code> is the number of trials and <code>T</code> is the number of time steps.</p> In\u00a0[13]: Copied! <pre>x = model.simulate(random.PRNGKey(0), n=100)\n\nplt.plot(jnp.arange(T) * dt, x[0, :, 0], label=\"target\")\nplt.plot(jnp.arange(T) * dt, x[0, :, 1], label=\"cursor\")\nplt.legend()\nplt.xlabel(\"Time [s]\")\nplt.ylabel(\"Position [arcmin]\")\nplt.show()\n</pre> x = model.simulate(random.PRNGKey(0), n=100)  plt.plot(jnp.arange(T) * dt, x[0, :, 0], label=\"target\") plt.plot(jnp.arange(T) * dt, x[0, :, 1], label=\"cursor\") plt.legend() plt.xlabel(\"Time [s]\") plt.ylabel(\"Position [arcmin]\") plt.show() In\u00a0[14]: Copied! <pre>x.shape\n</pre> x.shape Out[14]: <pre>(100, 500, 2)</pre> In\u00a0[15]: Copied! <pre>vels = jnp.diff(x, axis=1)\nlags, correls = xcorr(vels[...,1], vels[...,0], maxlags=120)\n\nplt.plot(lags, correls.mean(axis=0))\nplt.xlabel(\"Lag [s]\")\nplt.ylabel(\"Cross-correlation\")\n</pre> vels = jnp.diff(x, axis=1) lags, correls = xcorr(vels[...,1], vels[...,0], maxlags=120)  plt.plot(lags, correls.mean(axis=0)) plt.xlabel(\"Lag [s]\") plt.ylabel(\"Cross-correlation\") Out[15]: <pre>Text(0, 0.5, 'Cross-correlation')</pre> In\u00a0[16]: Copied! <pre>class BoundedActor(LQG):\n    def __init__(self, \n               sigma_target, \n               action_variability, \n               action_cost, \n               sigma_cursor):\n\n        dt = 1. / 60.\n\n        A = jnp.eye(2)\n        B = jnp.array([[0.], \n                      [dt]])\n\n        V = jnp.diag(jnp.array([1., action_variability]))\n\n        F = jnp.eye(2)\n        W = jnp.diag(jnp.array([sigma_target, sigma_cursor]))\n\n\n        Q = jnp.array([[1., -1.],\n                      [-1., 1]])\n\n        R = jnp.eye(1) * action_cost\n\n\n        super().__init__(A=A, B=B, F=F, V=V, W=W, Q=Q, R=R, T=T)\n</pre> class BoundedActor(LQG):     def __init__(self,                 sigma_target,                 action_variability,                 action_cost,                 sigma_cursor):          dt = 1. / 60.          A = jnp.eye(2)         B = jnp.array([[0.],                        [dt]])          V = jnp.diag(jnp.array([1., action_variability]))          F = jnp.eye(2)         W = jnp.diag(jnp.array([sigma_target, sigma_cursor]))           Q = jnp.array([[1., -1.],                       [-1., 1]])          R = jnp.eye(1) * action_cost           super().__init__(A=A, B=B, F=F, V=V, W=W, Q=Q, R=R, T=T) <p>We can now simulate data from the model given the four parameters. To do this efficiently, we <code>jit</code>-compile the simulation function.</p> <p>Some observations:</p> <ul> <li>An increase in action costs leads to an increased lag and decreased maximum correlation.</li> <li>An increase in perceptual uncertainty about the target leads to decreased correlation and increased lag, too, but the shape of the curves changes differently compared to the effect of the behavioral cost.</li> <li>Action variability does not change the lag, but decreases correlation overall.</li> <li>Perceptual uncertainty about the cursor does not change the shape of the CCGs at all, but does increase the mean squared error between target and response.</li> </ul> In\u00a0[17]: Copied! <pre>@jit  # jit-compile the simulation to speed up the data generation\ndef simulate_trajectories(sigma_target, action_cost, action_variability, sigma_cursor):\n    model = BoundedActor(\n        sigma_target=sigma_target,\n        action_variability=action_variability,\n        action_cost=action_cost,\n        sigma_cursor=sigma_cursor,\n    )\n\n    x = model.simulate(random.PRNGKey(0), n=100)\n\n    return x\n</pre> @jit  # jit-compile the simulation to speed up the data generation def simulate_trajectories(sigma_target, action_cost, action_variability, sigma_cursor):     model = BoundedActor(         sigma_target=sigma_target,         action_variability=action_variability,         action_cost=action_cost,         sigma_cursor=sigma_cursor,     )      x = model.simulate(random.PRNGKey(0), n=100)      return x In\u00a0[18]: Copied! <pre># Simulate data and store it in a DataFrame\ndata_trajectory = []\ndata_ccg = []\n\nsigma_target_list = [1.0, 10.0, 100.0]\naction_cost_list = [0.2, 1.0, 5.0]\naction_variability_list = [0.25, 0.5, 1.0]\nsigma_cursor_list = [1.0, 10.0, 100.0]\n\ntime_max = 500\ntime = jnp.arange(time_max) * dt\n\nfor sigma_target in sigma_target_list:\n    for action_cost in action_cost_list:\n        for action_variability in action_variability_list:\n            for sigma_cursor in sigma_cursor_list:\n                x = simulate_trajectories(\n                    sigma_target, action_cost, action_variability, sigma_cursor\n                )\n                for i, step in enumerate(x[0]):\n                    data_trajectory.append(\n                        [\n                            sigma_target,\n                            action_cost,\n                            action_variability,\n                            sigma_cursor,\n                            \"target\",\n                            time[i].item(),\n                            step[0].item(),\n                        ]\n                    )\n                    data_trajectory.append(\n                        [\n                            sigma_target,\n                            action_cost,\n                            action_variability,\n                            sigma_cursor,\n                            \"cursor\",\n                            time[i].item(),\n                            step[1].item(),\n                        ]\n                    )\n\n                vels = jnp.diff(x, axis=1)\n                lags, correls = xcorr(vels[..., 1], vels[..., 0], maxlags=120)\n                lags = lags / 60\n                correls = correls.mean(axis=0)\n\n                for lag, correl in zip(lags, correls):\n                    data_ccg.append(\n                        [\n                            sigma_target,\n                            action_cost,\n                            action_variability,\n                            sigma_cursor,\n                            lag.item(),\n                            correl.item(),\n                        ]\n                    )\n\ndf_trajectory = pd.DataFrame(\n    data_trajectory,\n    columns=[\n        \"sigma_target\",\n        \"action_cost\",\n        \"action_variability\",\n        \"sigma_cursor\",\n        \"value\",\n        \"time\",\n        \"position\",\n    ],\n)\ndf_ccg = pd.DataFrame(\n    data_ccg,\n    columns=[\"sigma_target\", \"action_cost\", \"action_variability\", \"sigma_cursor\", \"lag\", \"correl\"],\n)\n</pre> # Simulate data and store it in a DataFrame data_trajectory = [] data_ccg = []  sigma_target_list = [1.0, 10.0, 100.0] action_cost_list = [0.2, 1.0, 5.0] action_variability_list = [0.25, 0.5, 1.0] sigma_cursor_list = [1.0, 10.0, 100.0]  time_max = 500 time = jnp.arange(time_max) * dt  for sigma_target in sigma_target_list:     for action_cost in action_cost_list:         for action_variability in action_variability_list:             for sigma_cursor in sigma_cursor_list:                 x = simulate_trajectories(                     sigma_target, action_cost, action_variability, sigma_cursor                 )                 for i, step in enumerate(x[0]):                     data_trajectory.append(                         [                             sigma_target,                             action_cost,                             action_variability,                             sigma_cursor,                             \"target\",                             time[i].item(),                             step[0].item(),                         ]                     )                     data_trajectory.append(                         [                             sigma_target,                             action_cost,                             action_variability,                             sigma_cursor,                             \"cursor\",                             time[i].item(),                             step[1].item(),                         ]                     )                  vels = jnp.diff(x, axis=1)                 lags, correls = xcorr(vels[..., 1], vels[..., 0], maxlags=120)                 lags = lags / 60                 correls = correls.mean(axis=0)                  for lag, correl in zip(lags, correls):                     data_ccg.append(                         [                             sigma_target,                             action_cost,                             action_variability,                             sigma_cursor,                             lag.item(),                             correl.item(),                         ]                     )  df_trajectory = pd.DataFrame(     data_trajectory,     columns=[         \"sigma_target\",         \"action_cost\",         \"action_variability\",         \"sigma_cursor\",         \"value\",         \"time\",         \"position\",     ], ) df_ccg = pd.DataFrame(     data_ccg,     columns=[\"sigma_target\", \"action_cost\", \"action_variability\", \"sigma_cursor\", \"lag\", \"correl\"], ) In\u00a0[19]: Copied! <pre># Plot the data as interactive Altair chart\nradio1 = alt.binding_radio(\n    options=sigma_target_list,\n    name=\"Perceptual uncertainty (target): \",\n)\nselection1 = alt.selection_point(\n    value=10.0,\n    fields=[\"sigma_target\"],\n    bind=radio1,\n)\n\nradio2 = alt.binding_radio(options=action_cost_list, name=\"Behavioral costs: \")\nselection2 = alt.selection_point(\n    value=1.0,\n    fields=[\"action_cost\"],\n    bind=radio2,\n)\n\nradio3 = alt.binding_radio(options=action_variability_list, name=\"Action variability: \")\nselection3 = alt.selection_point(\n    value=0.5,\n    fields=[\"action_variability\"],\n    bind=radio3,\n)\n\nradio4 = alt.binding_radio(options=sigma_cursor_list, name=\"Perceptual uncertainty (cursor): \")\nselection4 = alt.selection_point(\n    value=10.0,\n    fields=[\"sigma_cursor\"],\n    bind=radio4,\n)\n\nlines_trajectory = (\n    alt.Chart(df_trajectory)\n    .mark_line()\n    .encode(\n        x=\"time:Q\",\n        y=alt.Y(\"position\").scale(domain=(-30, 30)),\n        color=alt.Color(\"value\").sort(\"descending\"),\n        tooltip=[\"time\", \"position\"],\n    )\n    .add_params(selection4, selection3, selection2, selection1)\n    .transform_filter(selection1 &amp; selection2 &amp; selection3 &amp; selection4)\n    .properties(title=\"Trajectory\")\n)\n\nlines_ccg = (\n    alt.Chart(df_ccg)\n    .mark_line()\n    .encode(\n        x=\"lag:Q\",\n        y=alt.Y(\"correl:Q\").scale(domain=(-0.02, 0.1)),\n        color=alt.value(\"#2CA02C\"),\n        tooltip=[\"lag\", \"correl\"],\n    )\n    .add_params(selection4, selection3, selection2, selection1)\n    .transform_filter(selection1 &amp; selection2 &amp; selection3 &amp; selection4)\n    .properties(title=\"Cross-correlogram\")\n)\n\nchart = lines_trajectory | lines_ccg\ndisplay(chart)\n</pre> # Plot the data as interactive Altair chart radio1 = alt.binding_radio(     options=sigma_target_list,     name=\"Perceptual uncertainty (target): \", ) selection1 = alt.selection_point(     value=10.0,     fields=[\"sigma_target\"],     bind=radio1, )  radio2 = alt.binding_radio(options=action_cost_list, name=\"Behavioral costs: \") selection2 = alt.selection_point(     value=1.0,     fields=[\"action_cost\"],     bind=radio2, )  radio3 = alt.binding_radio(options=action_variability_list, name=\"Action variability: \") selection3 = alt.selection_point(     value=0.5,     fields=[\"action_variability\"],     bind=radio3, )  radio4 = alt.binding_radio(options=sigma_cursor_list, name=\"Perceptual uncertainty (cursor): \") selection4 = alt.selection_point(     value=10.0,     fields=[\"sigma_cursor\"],     bind=radio4, )  lines_trajectory = (     alt.Chart(df_trajectory)     .mark_line()     .encode(         x=\"time:Q\",         y=alt.Y(\"position\").scale(domain=(-30, 30)),         color=alt.Color(\"value\").sort(\"descending\"),         tooltip=[\"time\", \"position\"],     )     .add_params(selection4, selection3, selection2, selection1)     .transform_filter(selection1 &amp; selection2 &amp; selection3 &amp; selection4)     .properties(title=\"Trajectory\") )  lines_ccg = (     alt.Chart(df_ccg)     .mark_line()     .encode(         x=\"lag:Q\",         y=alt.Y(\"correl:Q\").scale(domain=(-0.02, 0.1)),         color=alt.value(\"#2CA02C\"),         tooltip=[\"lag\", \"correl\"],     )     .add_params(selection4, selection3, selection2, selection1)     .transform_filter(selection1 &amp; selection2 &amp; selection3 &amp; selection4)     .properties(title=\"Cross-correlogram\") )  chart = lines_trajectory | lines_ccg display(chart) In\u00a0[20]: Copied! <pre># Simulate data and store it in a DataFrame\ndata_walk = []\n\nsigma_pos_list = [0.0, 0.5, 1.0, 2.0]\nsigma_vel_list = [0.0, 0.5, 1.0, 2.0]\n\nnum_steps = 100\nnum_sims = 20\n\nkey_s, key_v = random.split(random.PRNGKey(123))\nx_true = jnp.cumsum(random.normal(key_s, shape=(num_steps, num_sims)), axis=0)\n\nfor sim_idx, sim in enumerate(x_true.T):\n    for step_idx, step in enumerate(sim):\n        data_walk.append([\"true\", 1, 0, sim_idx, step_idx, step.item()])\n\nfor sigma_pos in sigma_pos_list:\n    for sigma_vel in sigma_vel_list:\n        v = jnp.cumsum(random.normal(key_v, shape=(num_steps, num_sims)) * sigma_vel, axis=0)\n        x = jnp.cumsum(\n            random.normal(key_s, shape=(num_steps, num_sims)) * sigma_pos + dt * v, axis=0\n        )\n\n        for sim_idx, sim in enumerate(x.T):\n            for step_idx, step in enumerate(sim):\n                data_walk.append(\n                    [\"subjective\", sigma_pos, sigma_vel, sim_idx, step_idx, step.item()]\n                )\n\ndf_walk = pd.DataFrame(\n    data_walk, columns=[\"kind\", \"sigma_pos\", \"sigma_vel\", \"simulation\", \"step\", \"position\"]\n)\n</pre> # Simulate data and store it in a DataFrame data_walk = []  sigma_pos_list = [0.0, 0.5, 1.0, 2.0] sigma_vel_list = [0.0, 0.5, 1.0, 2.0]  num_steps = 100 num_sims = 20  key_s, key_v = random.split(random.PRNGKey(123)) x_true = jnp.cumsum(random.normal(key_s, shape=(num_steps, num_sims)), axis=0)  for sim_idx, sim in enumerate(x_true.T):     for step_idx, step in enumerate(sim):         data_walk.append([\"true\", 1, 0, sim_idx, step_idx, step.item()])  for sigma_pos in sigma_pos_list:     for sigma_vel in sigma_vel_list:         v = jnp.cumsum(random.normal(key_v, shape=(num_steps, num_sims)) * sigma_vel, axis=0)         x = jnp.cumsum(             random.normal(key_s, shape=(num_steps, num_sims)) * sigma_pos + dt * v, axis=0         )          for sim_idx, sim in enumerate(x.T):             for step_idx, step in enumerate(sim):                 data_walk.append(                     [\"subjective\", sigma_pos, sigma_vel, sim_idx, step_idx, step.item()]                 )  df_walk = pd.DataFrame(     data_walk, columns=[\"kind\", \"sigma_pos\", \"sigma_vel\", \"simulation\", \"step\", \"position\"] ) In\u00a0[21]: Copied! <pre># Plot the data as interactive Altair chart\nradio1 = alt.binding_radio(options=sigma_pos_list, name=\"Subjective position std: \")\nselection1 = alt.selection_point(\n    value=1,\n    fields=[\"sigma_pos\"],\n    bind=radio1,\n)\n\nradio2 = alt.binding_radio(options=sigma_vel_list, name=\"Subjective velocity std: \")\nselection2 = alt.selection_point(\n    value=0,\n    fields=[\"sigma_vel\"],\n    bind=radio2,\n)\n\nlines_true = (\n    alt.Chart(df_walk.query(\"kind == 'true'\"))\n    .mark_line()\n    .encode(\n        x=\"step:Q\",\n        y=\"position:Q\",\n        color=alt.Color(\"simulation:N\").scale(scheme=\"rainbow\"),\n        tooltip=[\"simulation\", \"step\", \"position\"],\n    )\n    .properties(title=\"True Random Walk\")\n)\n\nlines_subjective = (\n    alt.Chart(df_walk.query(\"kind == 'subjective'\"))\n    .mark_line()\n    .encode(\n        x=\"step:Q\",\n        y=\"position:Q\",\n        color=alt.Color(\"simulation:N\"),\n        detail=[\"sigma_pos\", \"sigma_vel\"],\n        tooltip=[\"simulation\", \"step\", \"position\"],\n    )\n    .add_params(selection2, selection1)\n    .transform_filter(selection1)\n    .transform_filter(selection2)\n    .properties(title=\"Subjective Random Walk\")\n)\n\nchart = (lines_true | lines_subjective).resolve_scale(y=\"shared\")\ndisplay(chart)\n</pre> # Plot the data as interactive Altair chart radio1 = alt.binding_radio(options=sigma_pos_list, name=\"Subjective position std: \") selection1 = alt.selection_point(     value=1,     fields=[\"sigma_pos\"],     bind=radio1, )  radio2 = alt.binding_radio(options=sigma_vel_list, name=\"Subjective velocity std: \") selection2 = alt.selection_point(     value=0,     fields=[\"sigma_vel\"],     bind=radio2, )  lines_true = (     alt.Chart(df_walk.query(\"kind == 'true'\"))     .mark_line()     .encode(         x=\"step:Q\",         y=\"position:Q\",         color=alt.Color(\"simulation:N\").scale(scheme=\"rainbow\"),         tooltip=[\"simulation\", \"step\", \"position\"],     )     .properties(title=\"True Random Walk\") )  lines_subjective = (     alt.Chart(df_walk.query(\"kind == 'subjective'\"))     .mark_line()     .encode(         x=\"step:Q\",         y=\"position:Q\",         color=alt.Color(\"simulation:N\"),         detail=[\"sigma_pos\", \"sigma_vel\"],         tooltip=[\"simulation\", \"step\", \"position\"],     )     .add_params(selection2, selection1)     .transform_filter(selection1)     .transform_filter(selection2)     .properties(title=\"Subjective Random Walk\") )  chart = (lines_true | lines_subjective).resolve_scale(y=\"shared\") display(chart) <p>We now define a class for the subjective actor. We need to distinguish between the matrices of the actual dynamical system of the experiment and those of the subjective internal model of the agent. This is possible in the <code>lqg</code> package by defining a <code>Dynamics</code> and an <code>Actor</code> object and combining them into a <code>System</code>.</p> In\u00a0[22]: Copied! <pre>class SubjectiveActor(System):\n    def __init__(self, \n               sigma_target, \n               action_variability, \n               action_cost, \n               sigma_cursor,\n               sigma_s, \n               sigma_v):\n\n        dt = 1. / 60.\n\n        # true dynamical system (same as above)\n        A = jnp.eye(2)\n        B = jnp.array([[0.], [dt]])\n        V = jnp.diag(jnp.array([1., action_variability]))\n\n        C = jnp.eye(2)\n        W = jnp.diag(jnp.array([sigma_target, sigma_cursor]))\n\n        dynamics = Dynamics(A, B, C, V, W, T=T)\n\n        # subjective dynamical system parameters\n        A_subj = jnp.array([[1., 0., dt],  # target position\n                            [0., 1., 0.],  # cursor position\n                            [0., 0., 1.]]) # target velocity\n        B_subj = jnp.array([[0.], [dt], [0.]])\n        C_subj = jnp.array([[1., 0., 0.],\n                            [0., 1., 0.]])\n        V_subj = jnp.diag(jnp.array([sigma_s, action_variability, sigma_v]))\n\n        # cost function\n        Q = jnp.array([[1., -1., 0.],\n                    [-1., 1., 0.],\n                    [0., 0., 0.]])\n\n        R = jnp.eye(1) * action_cost\n\n        actor = Actor(A_subj, B_subj, C_subj, V_subj, W, Q, R, T=T)\n\n        super().__init__(actor=actor, dynamics=dynamics)\n</pre> class SubjectiveActor(System):     def __init__(self,                 sigma_target,                 action_variability,                 action_cost,                 sigma_cursor,                sigma_s,                 sigma_v):          dt = 1. / 60.          # true dynamical system (same as above)         A = jnp.eye(2)         B = jnp.array([[0.], [dt]])         V = jnp.diag(jnp.array([1., action_variability]))          C = jnp.eye(2)         W = jnp.diag(jnp.array([sigma_target, sigma_cursor]))          dynamics = Dynamics(A, B, C, V, W, T=T)          # subjective dynamical system parameters         A_subj = jnp.array([[1., 0., dt],  # target position                             [0., 1., 0.],  # cursor position                             [0., 0., 1.]]) # target velocity         B_subj = jnp.array([[0.], [dt], [0.]])         C_subj = jnp.array([[1., 0., 0.],                             [0., 1., 0.]])         V_subj = jnp.diag(jnp.array([sigma_s, action_variability, sigma_v]))          # cost function         Q = jnp.array([[1., -1., 0.],                     [-1., 1., 0.],                     [0., 0., 0.]])          R = jnp.eye(1) * action_cost          actor = Actor(A_subj, B_subj, C_subj, V_subj, W, Q, R, T=T)          super().__init__(actor=actor, dynamics=dynamics) In\u00a0[23]: Copied! <pre>@jit  # jit-compile the simulation to speed up the data generation\ndef simulate_subjective_actor(sigma_s, sigma_v):\n    model = SubjectiveActor(\n        action_variability=0.5,\n        action_cost=0.05,\n        sigma_target=6.0,\n        sigma_cursor=3.0,\n        sigma_s=sigma_s,\n        sigma_v=sigma_v,\n    )\n    x = model.simulate(random.PRNGKey(0), x0=jnp.zeros(2), n=100)\n\n    return x\n</pre> @jit  # jit-compile the simulation to speed up the data generation def simulate_subjective_actor(sigma_s, sigma_v):     model = SubjectiveActor(         action_variability=0.5,         action_cost=0.05,         sigma_target=6.0,         sigma_cursor=3.0,         sigma_s=sigma_s,         sigma_v=sigma_v,     )     x = model.simulate(random.PRNGKey(0), x0=jnp.zeros(2), n=100)      return x In\u00a0[24]: Copied! <pre># Simulate data and store it in a DataFrame\ndata_trajectory_2 = []\ndata_ccg_2 = []\n\nfor sigma_pos in sigma_pos_list:\n    for sigma_vel in sigma_vel_list:\n        x = simulate_subjective_actor(sigma_pos, sigma_vel)\n        for t, step in zip(time, x[0]):\n            data_trajectory_2.append([sigma_pos, sigma_vel, \"target\", t.item(), step[0].item()])\n            data_trajectory_2.append([sigma_pos, sigma_vel, \"cursor\", t.item(), step[1].item()])\n\n        vels = jnp.diff(x, axis=1)\n        lags, correls = xcorr(vels[..., 1], vels[..., 0], maxlags=120)\n        lags = lags / 60\n        correls = correls.mean(axis=0)\n\n        for lag, correl in zip(lags, correls):\n            data_ccg_2.append([sigma_pos, sigma_vel, lag.item(), correl.item()])\n\ndf_trajectory_2 = pd.DataFrame(\n    data_trajectory_2, columns=[\"sigma_pos\", \"sigma_vel\", \"value\", \"time\", \"position\"]\n)\ndf_ccg_2 = pd.DataFrame(data_ccg_2, columns=[\"sigma_pos\", \"sigma_vel\", \"lag\", \"correl\"])\n</pre> # Simulate data and store it in a DataFrame data_trajectory_2 = [] data_ccg_2 = []  for sigma_pos in sigma_pos_list:     for sigma_vel in sigma_vel_list:         x = simulate_subjective_actor(sigma_pos, sigma_vel)         for t, step in zip(time, x[0]):             data_trajectory_2.append([sigma_pos, sigma_vel, \"target\", t.item(), step[0].item()])             data_trajectory_2.append([sigma_pos, sigma_vel, \"cursor\", t.item(), step[1].item()])          vels = jnp.diff(x, axis=1)         lags, correls = xcorr(vels[..., 1], vels[..., 0], maxlags=120)         lags = lags / 60         correls = correls.mean(axis=0)          for lag, correl in zip(lags, correls):             data_ccg_2.append([sigma_pos, sigma_vel, lag.item(), correl.item()])  df_trajectory_2 = pd.DataFrame(     data_trajectory_2, columns=[\"sigma_pos\", \"sigma_vel\", \"value\", \"time\", \"position\"] ) df_ccg_2 = pd.DataFrame(data_ccg_2, columns=[\"sigma_pos\", \"sigma_vel\", \"lag\", \"correl\"]) In\u00a0[25]: Copied! <pre># Plot the data as interactive Altair chart\nradio1 = alt.binding_radio(options=sigma_pos_list, name=\"Subjective position std: \")\nselection1 = alt.selection_point(\n    value=1,\n    fields=[\"sigma_pos\"],\n    bind=radio1,\n)\n\nradio2 = alt.binding_radio(options=sigma_vel_list, name=\"Subjective velocity std: \")\nselection2 = alt.selection_point(\n    value=0,\n    fields=[\"sigma_vel\"],\n    bind=radio2,\n)\n\nlines_trajectory = (\n    alt.Chart(df_trajectory_2)\n    .mark_line()\n    .encode(\n        x=\"time:Q\",\n        y=alt.Y(\"position\").scale(domain=(-30, 30)),\n        color=alt.Color(\"value\").sort(\"descending\"),\n        tooltip=[\"time\", \"position\"],\n    )\n    .add_params(selection2, selection1)\n    .transform_filter(selection1 &amp; selection2)\n    .properties(title=\"Trajectory\")\n)\n\nlines_ccg = (\n    alt.Chart(df_ccg_2)\n    .mark_line()\n    .encode(\n        x=\"lag:Q\",\n        y=alt.Y(\"correl:Q\").scale(domain=(-0.02, 0.1)),\n        color=alt.value(\"#2CA02C\"),\n        tooltip=[\"lag\", \"correl\"],\n    )\n    .add_params(selection2, selection1)\n    .transform_filter(selection1 &amp; selection2)\n    .properties(title=\"Cross-correlogram\")\n)\n\nchart = lines_trajectory | lines_ccg\ndisplay(chart)\n</pre> # Plot the data as interactive Altair chart radio1 = alt.binding_radio(options=sigma_pos_list, name=\"Subjective position std: \") selection1 = alt.selection_point(     value=1,     fields=[\"sigma_pos\"],     bind=radio1, )  radio2 = alt.binding_radio(options=sigma_vel_list, name=\"Subjective velocity std: \") selection2 = alt.selection_point(     value=0,     fields=[\"sigma_vel\"],     bind=radio2, )  lines_trajectory = (     alt.Chart(df_trajectory_2)     .mark_line()     .encode(         x=\"time:Q\",         y=alt.Y(\"position\").scale(domain=(-30, 30)),         color=alt.Color(\"value\").sort(\"descending\"),         tooltip=[\"time\", \"position\"],     )     .add_params(selection2, selection1)     .transform_filter(selection1 &amp; selection2)     .properties(title=\"Trajectory\") )  lines_ccg = (     alt.Chart(df_ccg_2)     .mark_line()     .encode(         x=\"lag:Q\",         y=alt.Y(\"correl:Q\").scale(domain=(-0.02, 0.1)),         color=alt.value(\"#2CA02C\"),         tooltip=[\"lag\", \"correl\"],     )     .add_params(selection2, selection1)     .transform_filter(selection1 &amp; selection2)     .properties(title=\"Cross-correlogram\") )  chart = lines_trajectory | lines_ccg display(chart) <p>For the models from the paper, I have defined classes with the corresponding names in <code>lqg.tracking</code>.</p> In\u00a0[26]: Copied! <pre>true_params = dict(sigma_target=25., \n                   action_variability=0.5, \n                   action_cost=0.5, \n                   sigma_cursor=3.)\n\n# simulate some data\nmodel = BoundedActor(**true_params)\nx = model.simulate(random.PRNGKey(123), n=50)\n</pre> true_params = dict(sigma_target=25.,                     action_variability=0.5,                     action_cost=0.5,                     sigma_cursor=3.)  # simulate some data model = BoundedActor(**true_params) x = model.simulate(random.PRNGKey(123), n=50) In\u00a0[27]: Copied! <pre># log likelihood of one parameter, keeping others constant at the true value\ndef ll(sigma):\n    return BoundedActor(sigma_target=sigma, \n                      action_variability=true_params[\"action_variability\"], \n                      action_cost=true_params[\"action_cost\"],\n                      sigma_cursor=true_params[\"sigma_cursor\"]\n                      ).log_likelihood(x).sum()\n                      \n# range of parameter values\nsigmas = jnp.linspace(5., 50.)\n\nplt.plot(sigmas, vmap(ll)(sigmas))\nplt.axvline(25.)\nplt.xlabel(r\"$\\sigma$\")\nplt.ylabel(r\"$\\log p(x \\mid \\sigma)$\")\nplt.title(\"Log likelihood\")\n</pre> # log likelihood of one parameter, keeping others constant at the true value def ll(sigma):     return BoundedActor(sigma_target=sigma,                        action_variability=true_params[\"action_variability\"],                        action_cost=true_params[\"action_cost\"],                       sigma_cursor=true_params[\"sigma_cursor\"]                       ).log_likelihood(x).sum()                        # range of parameter values sigmas = jnp.linspace(5., 50.)  plt.plot(sigmas, vmap(ll)(sigmas)) plt.axvline(25.) plt.xlabel(r\"$\\sigma$\") plt.ylabel(r\"$\\log p(x \\mid \\sigma)$\") plt.title(\"Log likelihood\") Out[27]: <pre>Text(0.5, 1.0, 'Log likelihood')</pre> <p>We could also do gradient-based optimization of the log likelihood function by using <code>jax</code>'s automatic differentiation tools.</p> In\u00a0[28]: Copied! <pre>grad(ll)(28.)\n</pre> grad(ll)(28.) Out[28]: <pre>Array(-0.71183217, dtype=float32, weak_type=True)</pre> In\u00a0[29]: Copied! <pre>def lqg_model(x):\n\n    # priors\n    action_variability = numpyro.sample(\"action_variability\", dist.HalfCauchy(1.))\n    action_cost = numpyro.sample(\"action_cost\", dist.HalfCauchy(1.))\n    sigma_target = numpyro.sample(\"sigma_target\", dist.HalfCauchy(50.))\n    sigma_cursor = numpyro.sample(\"sigma_cursor\", dist.HalfCauchy(15.))\n\n    # setup model\n    model = BoundedActor(action_variability=action_variability,\n                       action_cost=action_cost,\n                       sigma_target=sigma_target,\n                       sigma_cursor=sigma_cursor)\n\n    # likelihood\n    numpyro.sample(\"x\", model.conditional_distribution(x),\n                  obs=x[:, 1:])\n</pre> def lqg_model(x):      # priors     action_variability = numpyro.sample(\"action_variability\", dist.HalfCauchy(1.))     action_cost = numpyro.sample(\"action_cost\", dist.HalfCauchy(1.))     sigma_target = numpyro.sample(\"sigma_target\", dist.HalfCauchy(50.))     sigma_cursor = numpyro.sample(\"sigma_cursor\", dist.HalfCauchy(15.))      # setup model     model = BoundedActor(action_variability=action_variability,                        action_cost=action_cost,                        sigma_target=sigma_target,                        sigma_cursor=sigma_cursor)      # likelihood     numpyro.sample(\"x\", model.conditional_distribution(x),                   obs=x[:, 1:]) In\u00a0[30]: Copied! <pre>nuts_kernel = NUTS(lqg_model)\n\nmcmc = MCMC(nuts_kernel, num_warmup=500, num_samples=1000)\nmcmc.run(random.PRNGKey(0), x)\n</pre> nuts_kernel = NUTS(lqg_model)  mcmc = MCMC(nuts_kernel, num_warmup=500, num_samples=1000) mcmc.run(random.PRNGKey(0), x) <pre>sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1500/1500 [05:39&lt;00:00,  4.42it/s, 15 steps of size 4.00e-01. acc. prob=0.93] \n</pre> In\u00a0[31]: Copied! <pre>inference_data = az.from_numpyro(mcmc)\n\naz.plot_pair(inference_data, reference_values=true_params, kind=\"kde\")\n\nsummary = az.summary(inference_data)\n</pre> inference_data = az.from_numpyro(mcmc)  az.plot_pair(inference_data, reference_values=true_params, kind=\"kde\")  summary = az.summary(inference_data) <pre>arviz - WARNING - Shape validation failed: input_shape: (1, 1000), minimum_shape: (chains=2, draws=4)\n</pre> In\u00a0[32]: Copied! <pre>model = BoundedActor(**summary[\"mean\"].to_dict())\n\nx = model.simulate(random.PRNGKey(1), n=100)\n</pre> model = BoundedActor(**summary[\"mean\"].to_dict())  x = model.simulate(random.PRNGKey(1), n=100) In\u00a0[33]: Copied! <pre>d = model.belief_tracking_distribution(x)\nbelief_samples = d.sample(random.PRNGKey(0), sample_shape=(10,))\nbelief_mean = d.loc # get mean of belief distribution\n\nf, ax = plt.subplots(1, 2)\nax[0].plot(jnp.arange(500) / 60, x[1, :, 0], label=\"target\")\nax[0].plot(jnp.arange(1, 500) / 60, belief_mean[1, :, 0], label=\"belief about target\", color=\"C2\")\nax[0].plot(jnp.arange(1, 500) / 60, belief_mean[1, :, 1], label=\"belief about cursor\", color=\"C1\")\n\nfor i in range(10):\n    ax[0].plot(jnp.arange(1, 500) / 60, belief_samples[i, 1, :, 0],\n          color=\"C2\", alpha=0.1, zorder=-1)\n    ax[0].plot(jnp.arange(1, 500) / 60, belief_samples[i, 1, :, 1],\n          color=\"C1\", alpha=0.1, zorder=-1)\nax[0].set_ylim(-10, 50)\nax[0].set_title(\"Belief tracking\")\nax[0].set_xlabel(\"Time [s]\")\nax[0].set_ylabel(\"Position [arcmin]\")\nax[0].legend(frameon=False)\n\n# cross-correlograms with belief about cursor\nlags, correls = xcorr(jnp.diff(belief_mean[..., 1], axis=1),\n                      jnp.diff(x[..., 0], axis=1), maxlags=120)\n\nax[1].plot(lags / 60, correls.mean(axis=0), color=\"C1\")\n\n# cross-correlograms with belief about target\nlags, correls = xcorr(jnp.diff(belief_mean[..., 0], axis=1),\n                      jnp.diff(x[..., 0], axis=1), maxlags=120)\n\nax[1].plot(lags / 60, correls.mean(axis=0), color=\"C2\")\n\nax[1].set_ylim(-.03, .12)\nax[1].set_title(\"Cross-correlogram\")\nax[1].set_ylabel(\"Correlation\")\nax[1].set_xlabel(\"Time lag [s]\")\n\nf.tight_layout()\n</pre> d = model.belief_tracking_distribution(x) belief_samples = d.sample(random.PRNGKey(0), sample_shape=(10,)) belief_mean = d.loc # get mean of belief distribution  f, ax = plt.subplots(1, 2) ax[0].plot(jnp.arange(500) / 60, x[1, :, 0], label=\"target\") ax[0].plot(jnp.arange(1, 500) / 60, belief_mean[1, :, 0], label=\"belief about target\", color=\"C2\") ax[0].plot(jnp.arange(1, 500) / 60, belief_mean[1, :, 1], label=\"belief about cursor\", color=\"C1\")  for i in range(10):     ax[0].plot(jnp.arange(1, 500) / 60, belief_samples[i, 1, :, 0],           color=\"C2\", alpha=0.1, zorder=-1)     ax[0].plot(jnp.arange(1, 500) / 60, belief_samples[i, 1, :, 1],           color=\"C1\", alpha=0.1, zorder=-1) ax[0].set_ylim(-10, 50) ax[0].set_title(\"Belief tracking\") ax[0].set_xlabel(\"Time [s]\") ax[0].set_ylabel(\"Position [arcmin]\") ax[0].legend(frameon=False)  # cross-correlograms with belief about cursor lags, correls = xcorr(jnp.diff(belief_mean[..., 1], axis=1),                       jnp.diff(x[..., 0], axis=1), maxlags=120)  ax[1].plot(lags / 60, correls.mean(axis=0), color=\"C1\")  # cross-correlograms with belief about target lags, correls = xcorr(jnp.diff(belief_mean[..., 0], axis=1),                       jnp.diff(x[..., 0], axis=1), maxlags=120)  ax[1].plot(lags / 60, correls.mean(axis=0), color=\"C2\")  ax[1].set_ylim(-.03, .12) ax[1].set_title(\"Cross-correlogram\") ax[1].set_ylabel(\"Correlation\") ax[1].set_xlabel(\"Time lag [s]\")  f.tight_layout() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/overview/#overview","title":"Overview\u00b6","text":""},{"location":"tutorials/overview/#quick-intro-to-jax","title":"Quick intro to jax\u00b6","text":""},{"location":"tutorials/overview/#1-numpy-like-api-jaxnumpy","title":"1. NumPy-like API: <code>jax.numpy</code>\u00b6","text":"<p>JAX is a library that enables transformations of array-manipulating programs written with a NumPy-like API. You can think of JAX as differentiable NumPy that runs on accelerators. Many NumPy programs would run just as well in JAX if you substitute np for jnp.</p>"},{"location":"tutorials/overview/#2-automatic-differentiation-grad","title":"2. Automatic differentiation: <code>grad</code>\u00b6","text":"<p>You can think of <code>jax.grad</code> by analogy to the $\\nabla$ operator from vector calculus. Given a function $f(x)$, $\\nabla f$ represents the function that computes $f$\u2019s gradient, i.e.</p> <p>$$ (\\nabla f)(x)_i = \\frac{\\partial f}{\\partial x_i} (x). $$</p> <p>Analogously, <code>jax.grad(f)</code> is the function that computes the gradient, so <code>jax.grad(f)(x)</code> is the gradient of <code>f</code> at <code>x</code>.</p>"},{"location":"tutorials/overview/#3-easy-vectorization-vmap","title":"3. Easy vectorization: <code>vmap</code>\u00b6","text":"<p>In JAX, the <code>jax.vmap</code> transformation is designed to generate a vectorized implementation of a function automatically.</p>"},{"location":"tutorials/overview/#4-compilation-jit","title":"4. Compilation: <code>jit</code>\u00b6","text":"<p>You can use the XLA (accelerated linear algebra) compiler to compile your functions with <code>jax.jit</code>.</p>"},{"location":"tutorials/overview/#putting-perception-into-action-inverse-optimal-control-for-continuous-psychophysics","title":"Putting perception into action: Inverse optimal control for continuous psychophysics\u00b6","text":""},{"location":"tutorials/overview/#modeling-a-tracking-task-with-lqg-control","title":"Modeling a tracking task with LQG control\u00b6","text":"<p>The LQG control problem is defined by a linear-Gaussian stochastic dynamical system $$ \\mathbf x_{t+1} = A \\mathbf x_t + B \\mathbf u_t + V \\mathbf \\epsilon_t, \\; \\mathbf\\epsilon_t \\sim \\mathcal{N}(0, I), $$</p> <p>a linear-Gaussian observation model $$ \\mathbf y_t = C \\mathbf x_t + W \\mathbf \\eta_t, \\; \\mathbf\\eta_t \\sim \\mathcal{N}(0, I), $$</p> <p>and a quadratic cost function</p> <p>$$ J = \\sum_t \\mathbf x_t^T Q \\mathbf x_t + \\mathbf u_t^T R \\mathbf u_t. $$</p> <p>We assume that the actor solves the linear-quadratic Gaussian problem, i.e. computes the Kalman filter $K$ and the LQR control law $L$, which are the optimal solution under the quadratic cost function</p> <p>$$ J(u_{1:T}) =  \\sum_{t=1}^T \\mathbf x_t^T Q \\mathbf x_t + \\mathbf u_t^T R \\mathbf u_t. $$</p> <p>We start by defining the matrices $A, B, C, V, W, Q, R$ as <code>jax.numpy.array</code>s. according to our simple model of the continuous psychophysics tracking task:</p> <p>$$ A = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix},  \\; B = \\begin{bmatrix} 0 \\\\ dt \\end{bmatrix}, \\; C = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}, \\\\ V = \\begin{bmatrix} \\sigma_\\text{rw} &amp; 0 \\\\ 0 &amp; \\sigma_\\text{act} \\end{bmatrix},  \\; W = \\begin{bmatrix} \\sigma &amp; 0 \\\\ 0 &amp; \\sigma_\\text{cursor} \\end{bmatrix}, \\\\ Q = \\begin{bmatrix} 1 &amp; -1 \\\\ -1 &amp; 1 \\end{bmatrix}, \\; R = \\begin{bmatrix} c \\end{bmatrix}. $$</p>"},{"location":"tutorials/overview/#cross-correlograms","title":"Cross-correlograms\u00b6","text":"<p>We can also look at the correlation between the velocities of the target and the cursor at different time lags. This analysis is known as a cross-correlogram (Mulligan et al., 2013) and computes the average autocorrelation of the velocities of target and response.</p>"},{"location":"tutorials/overview/#influence-of-model-parameters","title":"Influence of model parameters\u00b6","text":"<p>To look a the influence of the different model parameters, we define a class that inherits from the <code>LQG</code> base class and defines the matrices given the four parameters.</p>"},{"location":"tutorials/overview/#subjective-internal-models","title":"Subjective internal models\u00b6","text":"<p>The dynamical system that defines the experiment (i.e. the target movement) might not be known to the agent. In that case, we can assume that the agent has their own subjective internal model, which may differ from the true model.</p> <p>For example, instead of the true random walk</p> <p>$$ x_{t+1} = x_t + \\sigma_\\text{rw} * \\epsilon_t,$$</p> <p>the agent could assume that there is a velocity component to the random walk</p> <p>$$ x_{t+1} = x_t + dt * v_t + \\sigma_s * \\epsilon_t \\\\ v_{t+1} = v_t + \\sigma_v * \\varepsilon_t. $$</p> <p>Let's simulate this and compare it to the random walk on position that was used in the experiment.</p>"},{"location":"tutorials/overview/#inverse-optimal-control","title":"Inverse optimal control\u00b6","text":""},{"location":"tutorials/overview/#likelihood-function","title":"Likelihood function\u00b6","text":"<p>To compute the likelihood</p> <p>$$ p(\\mathbf x_{1:T} \\mid \\theta) = \\prod_{i=1}^{T-1} p(\\mathbf x_{t+1} \\mid \\mathbf x_{1:t}, \\theta), $$</p> <p>we need to marginalize out the latent variables $\\hat{\\mathbf x}_{1:T}$ (the agent's internal beliefs). We can do this efficiently in the LQG setting by starting with our belief about the agent's belief $p(\\hat{\\mathbf x}_t \\mid \\mathbf x_{1:t})$ and then iteratively performing the following steps:</p> <ol> <li>Propagate the uncertainty about the agent's belief through the joint dynamical system of states and beliefs $$ p(\\hat{\\mathbf{x}}_{t+1}, \\mathbf x_{t+1} \\mid \\mathbf x_{1:t}, \\theta) =  \\int p(\\hat{\\mathbf{x}}_{t+1}, \\mathbf x_{t+1} \\mid \\hat{\\mathbf{x}}_{t}, \\mathbf x_{t}, \\theta) p(\\hat{\\mathbf{x}}_{t} \\mid \\mathbf x_{1:t}, \\theta) d \\hat{\\mathbf{x}}_{t} $$</li> <li>Marginalize over the agent's belief $$ p(\\mathbf x_{t+1} \\mid \\mathbf x_{1:t}, \\theta) = \\int p(\\hat{\\mathbf{x}}_{t+1}, \\mathbf x_{t+1} \\mid \\mathbf x_{1:t}, \\theta) d \\hat{\\mathbf x}_{t+1}$$</li> <li>Condition on the observed state $$ p(\\hat{\\mathbf{x}}_{t+1} \\mid \\mathbf x_{1:t+1}, \\theta) = \\frac{p(\\hat{\\mathbf{x}}_{t+1}, \\mathbf x_{t+1} \\mid \\mathbf x_{1:t}, \\theta)}{p(\\mathbf x_{t+1} \\mid \\mathbf x_{1:t}, \\theta)} $$</li> </ol> <p>This gives us the contributions to the likelihood at each time step $p(\\mathbf x_{t+1} \\mid \\mathbf x_{1:t}, \\theta)$ (2.) and the distribution over the agent's belief for the next time step $p(\\hat{\\mathbf{x}}_{t+1} \\mid \\mathbf x_{1:t+1}, \\theta)$ (3.).</p> <p>This algorithm for computing the likelihood is implemented in the method <code>log_likelihood(x)</code> of our LQG class.</p>"},{"location":"tutorials/overview/#bayesian-inference-with-numpyro","title":"Bayesian inference with NumPyro\u00b6","text":"<p>In Bayesian inverse optimal control, we are interested in the posterior distribution of the model parameters given a trajectory</p> <p>$$ p(\\theta \\mid \\mathbf x_{1:T}) \\propto p(\\mathbf x_{1:T} \\mid \\theta) \\, p(\\theta). $$</p> <p>To sample from this posterior distribution, we will use <code>numpyro</code>, a probabilistic programming package powered by <code>jax</code>. For every random variable in the model (in our case the parameters and the observed data), we need to call <code>numpyro.sample</code> to define a name and a distribution.</p>"},{"location":"tutorials/overview/#belief-tracking","title":"Belief tracking\u00b6","text":"<p>Once we have fit the model, we can use it to do belief tracking: We compute the probabilitiy distribution</p> <p>$$ p(\\hat{\\mathbf x}_t | \\mathbf x_{1:t}), $$</p> <p>i.e. the researcher's uncertainty about the agent's belief.</p>"}]}